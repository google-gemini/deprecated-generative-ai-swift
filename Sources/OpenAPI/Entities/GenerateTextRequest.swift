// Generated by Create API
// https://github.com/CreateAPI/CreateAPI
//
// Copyright 2023 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation

/// Request to generate a text completion response from the model.
public struct GenerateTextRequest: Codable {
  /// The maximum cumulative probability of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Tokens are sorted based on their assigned probabilities so that only the most liekly tokens are considered. Top-k sampling directly limits the maximum number of tokens to consider, while Nucleus sampling limits number of tokens based on the cumulative probability. Note: The default value varies by model, see the `Model.top_p` attribute of the `Model` returned the `getModel` function.
  public var topP: Float?
  /// The set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a stop sequence. The stop sequence will not be included as part of the response.
  public var stopSequences: [String]?
  /// The maximum number of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Top-k sampling considers the set of `top_k` most probable tokens. Defaults to 40. Note: The default value varies by model, see the `Model.top_k` attribute of the `Model` returned the `getModel` function.
  public var topK: Int32?
  /// Controls the randomness of the output. Note: The default value varies by model, see the `Model.temperature` attribute of the `Model` returned the `getModel` function. Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied and creative, while a value closer to 0.0 will typically result in more straightforward responses from the model.
  public var temperature: Float?
  /// Number of generated responses to return. This value must be between [1, 8], inclusive. If unset, this will default to 1.
  public var candidateCount: Int32?
  /// Text given to the model as a prompt. The Model will use this TextPrompt to Generate a text completion.
  public var prompt: TextPrompt?
  /// The maximum number of tokens to include in a candidate. If unset, this will default to 64.
  public var maxOutputTokens: Int32?

  public init(topP: Float? = nil, stopSequences: [String]? = nil, topK: Int32? = nil, temperature: Float? = nil, candidateCount: Int32? = nil, prompt: TextPrompt? = nil, maxOutputTokens: Int32? = nil) {
    self.topP = topP
    self.stopSequences = stopSequences
    self.topK = topK
    self.temperature = temperature
    self.candidateCount = candidateCount
    self.prompt = prompt
    self.maxOutputTokens = maxOutputTokens
  }

  public init(from decoder: Decoder) throws {
    let values = try decoder.container(keyedBy: StringCodingKey.self)
    self.topP = try values.decodeIfPresent(Float.self, forKey: "topP")
    self.stopSequences = try values.decodeIfPresent([String].self, forKey: "stopSequences")
    self.topK = try values.decodeIfPresent(Int32.self, forKey: "topK")
    self.temperature = try values.decodeIfPresent(Float.self, forKey: "temperature")
    self.candidateCount = try values.decodeIfPresent(Int32.self, forKey: "candidateCount")
    self.prompt = try values.decodeIfPresent(TextPrompt.self, forKey: "prompt")
    self.maxOutputTokens = try values.decodeIfPresent(Int32.self, forKey: "maxOutputTokens")
  }

  public func encode(to encoder: Encoder) throws {
    var values = encoder.container(keyedBy: StringCodingKey.self)
    try values.encodeIfPresent(topP, forKey: "topP")
    try values.encodeIfPresent(stopSequences, forKey: "stopSequences")
    try values.encodeIfPresent(topK, forKey: "topK")
    try values.encodeIfPresent(temperature, forKey: "temperature")
    try values.encodeIfPresent(candidateCount, forKey: "candidateCount")
    try values.encodeIfPresent(prompt, forKey: "prompt")
    try values.encodeIfPresent(maxOutputTokens, forKey: "maxOutputTokens")
  }
}
